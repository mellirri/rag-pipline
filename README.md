# rag-pipline
RAG pipeline with Mistral

Этот проект реализует Retrieval-Augmented Generation (RAG) пайплайн с использованием модели Mistral через сервер Ollama. 
RAG сочетает извлечение релевантной информации из базы знаний и генерацию ответов на её основе.

## Возможности
- Обработка текстовых данных лекций.
- Создание векторного индекса с помощью FAISS.
- Генерация ответов на вопросы с использованием модели Mistral.

## Установка
1. Клонируйте репозиторий:
   ```git clone https://github.com/ваш_аккаунт/rag-pipeline.git```
   ```cd rag-pipeline```

2.  Установите зависимости
   ```pip install -r requirements.txt```

3. Убедитесь, что сервер Ollama запущен:
   ```ollama serve```

## Запуск
1. Поместите файл с лекциями в папку проекта и назовите его `lectures.txt`.
2. Запустите сервер RAG:
   ```python rag.py```
3. Отправьте запрос к серверу, чтобы протестировать:
   ```curl -X GET "http://127.0.0.1:8000/query?question=Что%20такое%20философия?"```

Ответ будет содержать:
answer: Ответ на ваш вопрос.
context: Текст из базы знаний, использованный для ответа.

### Требования
- Python 3.8+
- Установленный сервер Ollama
- Подключение к интернету для загрузки модели Mistral
